# 变分推断 (Variational Inference, VI)

机器学习中，有很多需要求后验概率的情况，求后验概率的过程被称为推断（inference）。





在计算贝叶斯公式时:
$$
P(z | x) = \frac{P(x | z) P(z)}{P(x)}
$$
其中，由全概率公式可知（联合概率分布计算边缘概率分布）：
$$
P(x) = \int_z p(x, z) dz
$$
对于简单分布，如 $z$ 的维度较低时，上述积分可能很好计算。而如果 $z$ 非常复杂，上述积分可能非常难以计算。因此，变分推断就是贝叶斯近似推断中的一类方法，即，把计算后验概率 $p(z|x)$ 巧妙的转化为求解优化问题。



对于 $P(z | x)$ （下图中蓝色的点），一定是在一个未知的概率分布空间（下图大的橘黄色椭圆）内的一个点，如果 $P(z | x)$ 难以求解，我们可以在这个概率空间內部找出来一个相对简单，并且能够替代 $P(z|x)$ 的一个子集（下图中绿色的椭圆，或者说是整个概率分布空间内的一个概率分布family）。

![image-20230527200802675](imgs/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/image-20230527200802675.png)

然而，如上图所示，即使我们能够找到绿色椭圆所示的一个更加简单的概率分布family，其中也有无数个概率分布，如果确定其中最优的一个能够替代蓝色点 $P(z | x)$ 的概率分布（假设用$q^*(z)$ 表示）呢？

当然，从图像上看，可以使用欧式距离。但是在变分推断中，使用的是 KL 散度来评估两个分布的相似程度。即：

- 在整个概率分布空间中，首先找到一个更加简单的概率分布 $Q$ 
- 对于 $Q$ 中的每个点（每个分布），计算其与目标分布 $P(z |x)$  的KL散度，找到与目标分布最接近的 $q^*(z)$ 即可

用最优化的语言描述上述过程，即为：
$$
q^*(z) = arg min_{q(z) \in Q} KL(q(z) || p(z|x))
$$
之后的贝叶斯计算，就可以用更加简单的 $q^*(z)$ 来代替 $P(z | x)$ 来进行计算。



## ELBO （证据下界）

何为证据？贝叶斯公式 $P(z | x) = \frac{P(x | z) P(z)}{P(x)}$ 中， 多$P(x)$ 取log ，即 $log(p(x))$ 即为证据 evidence 。

当我们用KL散度来作为变分推断中评价两个分布距离的标准时，此问题就变成了 Variational Bayes 问题。

展开KL项：
$$
q^*(z) = arg min_{q(z) \in Q} KL(q(z) || p(z|x))

\\ =

arg min_{q(z) \in Q} H(q(z), p(z|x)) - H(q(z))

\\=

arg min_{q(z) \in Q} -\int_z q(z) log(p(z|x)) dz + \int_z q(z) log(q(z)) dz


\\=

arg min_{q(z) \in Q} \int_z q(z)log[\frac{q(z)}{p(z|x)}] dz

\\=

arg min_{q(z) \in Q} - \int_z q(z) log[\frac{p(z|x)}{q(z)}] dz
$$
当我们尝试去计算最优解 $q^*(z)$ 时，发现其表达式中仍然存在 $p(z|x)$ ，而 $p(z | x)$ 却是我们最终想估计的分布，所以有些本末导致了。下面的计算就是如何替换掉 $p(z|x)$ :

由于
$$
KL(q(z) || p(z|x)) = \int_z q(z) log(q(z)) dz -\int_z q(z) log(p(z|x)) dz
$$
其中 $q(z), p(z|x)$ 都是关于 $z$ 的概率分布。因此 $\int_z q(z) f(z, \cdot) dz$ 计算的实际上是 $f(z, \cdot)$ 的期望。因此，上式等价于：
$$
KL(q(z) || p(z|x)) = \int_z q(z) log(q(z)) dz -\int_z q(z) log(p(z|x)) dz
\\=
E_q[log(q(z))] - E_q[log(p(z|x))]
$$
其中，
$$
E_q[log(p(z|x))]
\\=
E_q[log(\frac{p(x, z)}{p(x)})]
$$
即，
$$
KL(q(z) || p(z|x)) = \int_z q(z) log(q(z)) dz -\int_z q(z) log(p(z|x)) dz
\\=
E_q[log(q(z))] - E_q[log(p(z|x))]
\\=
E_q[log(q(z))] - E_q[\frac{p(x, z)}{p(x)}]
\\=
E_q[log(q(z))] - E_q[log(p(x, z))] + E_q[log(p(x))]
$$
由于 $E_q[log(p(x))]$ 与 $z$ 无关，因此该期望是个常数，即：
$$
KL(q(z) || p(z|x)) = E_q[log(q(z))] - E_q[log(p(x, z))] + log(p(x))
$$
其中，前两项
$$
E_q[log(q(z))] - E_q[log(p(x, z))] = -ELBO
$$
即为**负的**证据下界。即，
$$
ELBO(q) = E_q[log(p(x, z))] - E_q[log(q(z))]
$$
ELBO又可以表示成：
$$
ELBO(q) = E_q[log(p(x, z))] - E_q[log(q(z))]
\\=
E_q[log(p(x|z)p(z))] - E_q[log(q(z))]
\\=
E_q[log(p(x|z))] + E_q[log(p(z))] - E_q[log(q(z))]
\\=
E_q[log(p(x|z))] + E_q[\frac{log(p(z))}{log(q(z))}]
\\=
E_q[log(p(x|z))] + \int_z q(z) \frac{log(p(z))}{log(q(z))} dz
\\=
E_q[log(p(x|z))] - KL(q(z)||p(z))
$$
因此，我们的最优化目标函数是最小化 
$$
arg min_{q(z) \in Q} KL(q(z) || p(z|x)) = E_q[log(q(z))] - E_q[log(p(x, z))] + log(p(x))
\\=
-ELBO(q) + log(p(x))
$$
其中，$log(p(x))$ 为常数，因此需要最大化 $ELBO(q)$ ，即，最优化问题转化为了：
$$
q^*(z) = arg max_{q(z) \in Q} ELBO(q)
$$
此外，我们知道，在贝叶斯公式  $P(z | x) = \frac{P(x | z) P(z)}{P(x)}$ 中，$P(x)$ 称为证据 evidence。

并且:
$$
log(p(x)) = KL(q(z) || p(z|x)) + ELBO(q) \ge ELBO(q)
$$
因此，$ELBO(q)$ 就是 Evidence $log(p(x))$ 的下界，这是 $ELBO$ 名称的由来。