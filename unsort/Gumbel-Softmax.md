# Gumbel Softmax

本节开始之前，首先思考argmax和softmax：

对于 $softmax = [0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893]$ ，相当如下所示的离散型概率分布模型：

| X     | x=0        | x=1        | x=2        | x=3       | x=4        |
| ----- | ---------- | ---------- | ---------- | --------- | ---------- |
| **P** | 0.02948882 | 0.08015893 | 0.21789455 | 0.5922988 | 0.08015893 |

+ 题外话：

  上述概率分布的期望为：
  $$
  E(X) = \sum_{i=0}^{4} i \times p_i
  $$
  **（softmax概率分布的均值即为soft-argmax）**

如果我们想从上表所示的概率分布中采样出一些离散的样本 $X \in \{0, 1, 2, 3\} ^ N$ ，例如采样出的样本为  $\{3, 3, 2, 0, 3\}$ 。显然，由于 $p(x=3)$ 最大，因此采样出的样本也最多。此外，虽然 $p(x=0)$ 的概率较小，但也有一定几率被采样到。

当我们使用 argmax 时，其实也相当于进行采样，只不过每次采样出的样本都是概率最大的。显然，这种方式存在一定不足。如：

+ 以图像分类为例，预测出物体是汽车的概率是 $p(汽车) = 0.9$ ,是火车的概率是 $p(火车) = 0.1$ ，如果使用argmax的话，就只能获得预测类别是汽车这一种固定的类别，这种方式就偏离了概率分布本身所表示的意义了。
+ 在强化学习中，机器人能够按照前后左右四个方向移动，此时我们无法说概率最大的方向一定是最优的。

为了按照概率分布进行随机的采样，我们可以引入一定的随机性。首先介绍第一种方法 `np.random.choice` ：

+ 为了按照上表所示的离散型随机变量的概率分布进行采样，coding中可以使用如下代码：

  ```python
  import numpy as np
  
  x = [0, 1, 2, 3, 4]
  p = [0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893]
  
  result = []
  for _ in range(1000):
      sample = np.random.choice(x, 1, p=p)
      result.append(int(sample))
  
  result = np.array(result)
  
  print(np.sum(result == 3) / len(result))
  
  # 0.593
  ```

然而，这种采样方法是不可导的。如果我们需要在具有BP的模型中的某个中间环节引入采样过程，则无法计算梯度（如强化学习中采样后还需要让机器人移动，从而才能计算梯度）。

因此，是否有一种具有表达式的函数 $f$ ，能够使我们通过一些有具体表达式的计算来模拟该采样过程呢？如：

+ 第一次采样 ： $f(...) = 3$
+ 第二次采样 ： $f(...) = 3$
+ 第三次采样 ： $f(...) = 2$
+ ...

如果可以找到这样的函数 $f$ ，且 $f$ 可导，就能够实现我们的目标：既能够按照概率分布进行随机采样，又能够计算导数，使其能够加入到具有BP的模型中。



到这里，抛开Gumbel Softmax和Gumbel分布。再重新梳理一下当前的目标：

+ 为了采样一些离散值，以用于如强化学习等任务中。
+ 为什么不用argmax呢？1. argmax不可导。2. argmax仅采样 $p$ 最大的样本，无法按照真实的概率分布采样。



## 1 Gumbel 分布

Gumbel分布用来建模极大值或极小值的概率分布。Gumbel分布可以建模从高斯分布或指数分布中采样的极值。

+ 当建模极大值分布时的概率密度函数如下:
  $$
  f(x, \mu, \beta) = \frac{1}{\beta} e^{-z-e^{-z}}
  $$
  其中，$z = \frac{x-\mu}{\beta}$ 。$\mu$ 是位置参数，$\beta$ 是尺度参数。

  

+ 当建模极小值分布时的概率密度函数如下：
  $$
  f(x, \mu, \beta) = \frac{1}{\beta} e^{z-e^{z}}
  $$

当 $\mu = 0$ ，$\beta = 1$ 时，称为标准Gumbel分布。

标准Gumbel（极大值）分布的概率密度函数为：
$$
f(x) = e^{-x-e^{-x}}
$$
标准Gumbel（极大值）分布的累积分布函数为：
$$
F(x) = e^{-e^{-x}}
$$

## 2 从Gumbel分布中采样

回到之前确定的目标：

是否有一种具有表达式的函数 $f$ ，能够使我们通过一些有具体表达式的计算来模拟该采样过程呢？如：

+ 第一次采样 ： $f(...) = 3$
+ 第二次采样 ： $f(...) = 3$
+ 第三次采样 ： $f(...) = 2$
+ ...

如果可以找到这样的函数 $f$ ，且 $f$ 可导，就能够实现我们的目标：既能够按照概率分布进行随机采样，又能够计算导数，使其能够加入到具有BP的模型中。

对于这个目标，需要明确：

+ 对于固定表达式的函数，多次采样无法具有随机性

为了解决这个问题，可以向函数中引入一个随机噪声 $\epsilon$ 作为参数，即 $f(..., \epsilon)$ 。这个随机噪声在Gumbel Softmax中服从标准Gumbel （极大值）分布。当随机噪声 $\epsilon$ 发生变化时，函数值也会变化，因此才可以实现随机采样。

同时，引入了随机噪声 $\epsilon$ ，还需要保证采样出来的样本 $f(..., \epsilon)$ 服从于 $softmax = [0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893]$ 的概率分布。因此， $\epsilon$  也不是随便生成的随机变量。



Gumbel Softmax 中需要保证生成的 $\epsilon$ 服从于 Gumbel分布。既然有了Gumbel分布，怎么采样出来一系列随机变量来作为随机噪声呢？

对于概率密度函数或累积分布函数，我们都是已经有了一个随机变量，之后带到公式中来计算得到概率密度或累积概率。

如果我们指定一个概率值，怎么获取到一个随机变量呢？答案是可以使用反函数。即，通过累积分布函数（变量->概率）$F(x) = e^{-e^{-x}}$ 的反函数（概率->变量）来实现：
$$
F(y)^{-1} = -ln(-ln(x))
$$
有了可以按照概率获得随机变量的函数之后，又怎么生成概率 $x$ 呢？Gumbel Softmax采用的是均匀分布，即：
$$
x \sim U(0, 1)
$$
至此，我们可以生成一系列随机噪声 $\epsilon$ 了：
$$
\epsilon_i = -ln(-ln(x_i)) \\
x_i \sim U(0, 1)
$$
代码如下：

```python
import numpy as np

x = np.random.rand() # 生成1个服从 [0, 1) 均匀分布的随机数作为概率
epsilon = -np.log(-np.log(x + 1e-20) + 1e-20) # +1e-20 是为了防止当x=0时，出现log(0)

print(epsilon) # -0.4616422254510493
```



再回到最开始的目标：按照 $softmax = [0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893]$  的概率分布进行随机采样，并且随机采样的过程可以使用可导的函数 $f(..., \epsilon)$ 来进行，并且 $f(..., \epsilon)$ 采样出来的样本还需要服从  $softmax = [0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893]$ 的概率分布。

在上述最开始的目标当中，我们的问题就是：使用 argmax 只能够采样出来概率最大的样本，即 $argmax([0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893])$ 恒等于4。

为了便于理解，这里先不考虑使用可导的 $f(..., \epsilon)$ 进行采样，仍然考虑使用 argmax，如果能够使用 argmax，那么最不济的情况下我们也可以使用 soft-argmax 来对argmax计算梯度（[**nn中的不可导问题.md**](https://github.com/ielym/Notes/blob/main/unsort/nn%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%8F%AF%E5%AF%BC%E9%97%AE%E9%A2%98.md#4-argmax)）。既然我们现在可以获得一些列随机噪声了，那么能不能把这些随机噪声加到 softmax 中呢，即：
$$
softmax = [0.02948882 + \epsilon_0,  0.08015893 + \epsilon_1,  0.21789455 + \epsilon_2,  0.5922988 + \epsilon_3,  0.08015893 + \epsilon_4]
$$
可以尝试一下（虽然不正确），代码如下：

```python
import numpy as np

softmax = np.array([0.02948882,  0.08015893,  0.21789455,  0.5922988,  0.08015893])

xs = np.random.rand(len(softmax))
epsilon = -np.log(-np.log(xs + 1e-20) + 1e-20)

gumbel_softmax = softmax + epsilon

print(gumbel_softmax) # [ 0.59358353  0.32514174 -0.58618587 -0.12888364 -0.41790671]
```

如上述代码所示，我们此时引入随机噪声的 $gumbel\_softmax= [ 0.59358353,  0.32514174, -0.58618587, -0.12888364, -0.41790671]$ 。那么此时 argmax 的输出即为 0，可以实现随机采样而不是固定值了。

如果我们后续就使用 argmax + Gumbel噪声来采样，并使用 soft-argmax来对argmax求导，是不是问题已经解决了呢？ 讨论如下：

+ $gumbel\_softmax = softmax + \epsilon$ 显然已经改变了原始 $softmax$ 的概率分布了。

那么在Gumbel Softmax中应该怎么做呢：

+ Gumbel Softmax并不是直接把随机噪声 $\epsilon$ 加入到 softmax中，而是先对 $softmax$ 计算 $log$ ，之后再相加。即: 

$$
gumbel\_softmax = ln(softmax) + \epsilon
$$

整体流程如下：

+ 生成一些列随机噪声 $\epsilon$
+ 将随机噪声 $\epsilon$ 通过加法的方式引入到 $log(softmax)$ 中
+ 使用 argmax 进行采样，并且使用 soft-argmax的方式解决 argmax 导数无意义的问题。

其中，对于$softmax$ 计算的的 $i$ 个类别的概率 $p_i$ ，以及对应的随机噪声 $\epsilon_i$ ，以及为了生成 $\epsilon_i$ 的 $x_i$ ，以及Gumbel Softmax的输出 $g_i$。他们的关系如下：
$$
x_i \sim U(0, 1) \\
\epsilon_i = -ln(-ln(x_i)) \\
g_i = ln(p_i) + \epsilon_i = ln(p_i) -ln(-ln(x_i))
$$
其中， $0 \le x_i < 1$ , $ln(x_i) < 0$，$0 \le p_i \le 1$

所以，对于有  $n$ 个类别的分类任务，Gumbel Softmax 的输出为：
$$
gumbel\_softmax = [g_1, g_2, ..., g_i, ..., g_n]
$$
## 3 Gumbel Softmax 证明

那么，为什么 Gumbel Softmax这样做了之后，使用argmax对其进行采样，仍然能够保证采样出的样本的概率分布与原始softmax的概率分布呢？

证明如下：

假设 gumbel softmax 中第 $i$ 个位置（对应分类任务中的第 $i$ 个类别）的数值 $g_i$ 最大，即：
$$
ln(p_i) - ln(-ln(x_i)) >  ln(p_j)-ln(-ln(x_j)) \qquad \forall j\ne i
$$
进行化简：
$$
ln(p_i) - ln_(p_j) \gt ln(-ln(x_i)) - ln(-ln(x_j)) \\
ln(\frac{p_i}{p_j}) \gt ln(\frac{ln(x_i)}{ln(x_j)}) \\
\frac{p_i}{p_j} \gt \frac{ln(x_i)}{ln(x_j)} \\
\\
ln(x_j) \lt ln(x_i) \frac{p_j}{p_i} \\
e^{ln(x_j)} \lt e^{ln(x_i) \frac{p_j}{p_i}} = (e^{ln(x_i)})^{\frac{p_j}{p_i}} \\
x_j \lt x_i^{\frac{p_j}{p_i}}
$$
其中：

+ $x_i$ 和 $x_j$ 都为服从于 $U(0, 1)$ 的随机变量，因此 $0 \le x_i < 1$ ，$0 \le x_j < 1$

+ $p_i$ 和 $p_j$ 都为softmax的输出概率值，因此  $0 \le p_i \le 1$ ，$0 \le p_j \le 1$

+ 因此，$\frac{p_j}{p_i} \ge 0$ 。所以 $0 \lt x_i ^ {\frac{p_j}{p_i}} \le 1$

此外，考虑服从于 $U(0, 1)$ 的两个随机变量如 $a$ 和 $b = 0.4$ ，则 $a \gt b$ 的概率为 $0.6$ ，$a \lt b$ 的概率为 $0.4$ 。因此，$x_j \lt x_i^{\frac{p_j}{p_i}}$ 的概率即为 $x_i^{\frac{p_j}{p_i}}$ 。

那么，对于所有的 $j \ne i$ ，$x_j \lt x_i^{\frac{p_j}{p_i}}$ 都成立的概率为：
$$
\prod_{j \ne i} x_i^{\frac{p_j}{p_i}} = x_i ^ \frac{p_1 + p_2 + ... + p_{i-1} + p_{i+1} + ... + p_n}{p_i} = x_i ^ \frac{1 - p_i}{p_i}
$$
上式中，$p_i$ 和 $p_j$ 都为softmax的输出概率，可看作常数。而只有 $x_i$ 是服从于 $U(0, 1)$ 的随机变量。且上式表示概率值，因此取值范围在$[0,1]$之间。因此上式的期望为：
$$
\int_{0}^{1}x_i ^ \frac{1 - p_i}{p_i} dx_i = p_ix_i^{\frac{1}{p_i}}\Big|_{0}^{1} = p_i
$$
因此可以看出， Gumbel Softmax中的 $g_i$ 最大的概率的期望为 $p_i$ ，即采样出来的类别为 $i$ 的概率为 $p_i$ 。与原始softmax的分布一致。



到这里，我们能够使用 $argmax(gumbel\_softmax)$ 进行随机采样了，且此时的argmax采样出来的样本不是固定的，而是按照softmax的概率分布采样出的一些随机变量。但此时还是没有解决argmax导数无意义的问题，即还没有找到 $f(..., \epsilon)$ 。

Gumbel Softmax 为了解决这个问题，使用的方法类似于 soft-argmax，即利用 softmax 来作为 onehot(argmax) 的平滑近似:
$$
s_i = \frac{\frac{exp(ln(p_i) -ln(-ln(x_i)))}{t}}{ \sum_{j=1}^{n}  \frac{exp(ln(p_j) -ln(-ln(x_j)))}{t}}
$$
上式中 $t$ 的作用与 soft-argmax中的作用类似，都是为了对数值进行缩放，从而利用 $exp$ 进行指数映射拉开差距。因此 $t$ 越小，输出就越和 onehot 相似。
