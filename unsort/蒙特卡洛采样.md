# 蒙特卡洛方法 (Monte Carlo Method)

# 示例

## 计算 $\pi$

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-0dd8c70402b389dc97f1c6884f6b0ed8_720w.webp)

已知圆的面积公式为 $S = \pi r^2$ ，则上图 $1/4$ 圆的面积为 $S = \frac{1}{4} \pi r^2$ 。已知 $r = 1$ ，想要求 $\pi$ 。

按照蒙特卡洛采样法：

- 从正方形中随机采样一个点 $x$ 
- 如果 $x$ 落在了圆内，则接受 $x$ 。否则，拒绝 $x$ 
- 采样 $n$ 次，接受 $x$ 的次数为 $a$ ，拒绝 $x$ 的次数为 $b$ ，则四分之一圆的面积可以近似估计为 $a / (a + b)$ 

- 继而使用圆的表达式计算出 $\pi$ 

上述计算面积的方法，也可以扩展到计算不规则图形面积中。

---

## 三门问题

蒙特卡罗法也成为统计模拟方法（Statistical simulation method），是通过从概率模型的随机抽样进行近似数值计算的方法。

通过蒙特卡罗法求解概率分布的一个示例是求解三门问题：

三门问题(Monty Hall problem)大致出自美国的电视游戏节目Let's Make a Deal。问题名字来自该节目的主持人蒙提·霍尔(Monty Hall)。参赛者会看见三扇关闭了的门，其中一扇的后面有一辆汽车，选中后面有车的那扇门可赢得该汽车，另外两扇门后面则各藏有一只山羊。当参赛者选定了一扇门，但未去开启它的时候，节目主持人开启剩下两扇门的其中一扇，露出其中一只山羊。主持人其后会问参赛者要不要换另一扇仍然关上的门。问题是：换另一扇门会否增加参赛者赢得汽车的机率吗？答案是会。不换门的话，赢得汽车的几率是1/3。换门的话，赢得汽车的几率是2/3。

用蒙特卡罗法求解：

- 0,1,2分别代表三扇门
- 随机在0 ，1，2中选择一扇门表示奖品所在的门的编号
- 随机在0，1，2中选择一个索引作为用户选择的门

```python
import random

n = 1
win_rate_no_change = 0
win_rate_change = 0
while True:

    answer_door = random.randint(0, 2)
    select_door = random.randint(0, 2)

    # 不换门
    if select_door == answer_door:
        win_rate_no_change += 1
    else:
        # 随机选择一个不是用户选择的门且不是中奖的门
        doors = [0, 1, 2]
        doors.remove(answer_door)
        doors.remove(select_door)
        remove_door = random.choice(doors)

        # 移除一个门
        rest_doors = [0, 1, 2]
        rest_doors.remove(remove_door)

        # 换门
        final_door = rest_doors[0] if rest_doors[1] == select_door else rest_doors[1]

        if final_door == answer_door:
            win_rate_change += 1

    print(f'次数 : {n}, 换门 : {win_rate_change / n}, 不换门 : {win_rate_no_change / n}')

    n += 1
```

![image-20231216235243922](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/image-20231216235243922.png)

可以看出，可以使用采样法来近似估计赢得比赛的几率。

---

# 引入

从上面的示例可以看出，其共同点都是通过大量的随机样本去了解一个系统，进而得到要计算的值。也可以看出MC的应用场景：当所求解的问题是某种随机时间出现的概率，或是某个随机变量的期望时，通过实验方法进行采样，以时间出现的频率作为随机时间的概率，或得到随机i扮靓的某些数字特征（如积分等），并将其作为问题的解。

所以使用MC方法需要进行采样，那就需要构造一个合适的概率模型，使其某些统计量正好是待求问题的解，并作为待求量的估计值。

---

因此，蒙特卡罗法要解决的问题是：假设概率分布的定义已知，通过抽样获得概率分布的随机样本，并通过得到的随机样本对概率分布的特征进行分析。比如，从样本得到经验分布，从而估计总体分布；或者从样本计算出样本均值，从而估计总体期望。所以蒙特卡罗法的核心是随机抽样(random sampling)。

# 随机抽样方法

## 拒接-接受采样

三门问题或计算 $\pi$ 的示例中，我们可以很容易的构建出采样方法，但是实际问题中可能 $p(x)$ 根本不知道是什么，或 $p(x)$ 是一个很复杂的表达式，没法直接抽样，这时候就需要使用拒绝-接受采样。拒绝接受采样适用于 $p(x)$ 极度复杂不规则的情况。

由于 $p(x)$ 没法直接采样，那么可以找一个简单的分布 $q(x)$ 作为媒介，这个媒介的专业术语叫做建议分布 `Proposal Distribution` 。$q(x)$ 比如可以是均匀分布，高斯分布等，并且 $cq(x) \ge p(x)$ ，其中 $c \gt 0$  ，如下图所示：

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-c6015aa0d342bc1598c460bbd9fb8311_720w.webp)

---

这个问题也很好理解：

- 使用一个常数 $C$ 对 $q(x)$ 进行拉伸，保证 $C \times q(x)$ 在$p(x)$ 的上方（同时也希望 $C$ 越小越好，即保证能够罩住 $p(x)$ ，又不能太大），如下图所示：

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-19a1a9400f65eb2b99eb27a5384f62c9_720w.webp)

- 向简单分布（蓝色曲线）围成的区域内投点。如何投点？

  - 根据简单分布，随机采样出样本 $x$  ，并计算 $C \times q(x)$ ：

    ![image-20230627121850381](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/image-20230627121850381.png)

  - 对采样的样本 $(x, C \times q(x))$ 的 $y$ 坐标进行随机移动，使 $y$ 坐标随机变化到上图绿色竖线上的任意一点。该过程可以表示为把纵坐标变换为 $U \times c \times q(x)$ ，$U \in [0, 1]$ 即可实现在上图绿色竖线上移动。

- 如果 $c \times q(x) \times u < p(x)$ ，即 $u < \frac{p(x)}{q(x) \times c}$ ，则接受采样样本 $x$ ，否则拒绝该采样

很容易看出，对于任意采样得到的 $x$ ：

- 接受采样的概率是 $\frac{p(x)}{cq(x)}$
- 拒绝采样的概率是 $1 - \frac{p(x)}{cq(x)}$

## 重采样技术

重采样方法（Reparameterization trick）在VAE中经常使用，这种方法适用于 $p(x)$ 是常见的连续分布，如正态分布，t分布等。

VAE使用重采样方法的目的是为了让网络能够完成反向传播，具体采样流程：

- 想从 $N(\mu, \sigma^2)$ 中采样出一个 $Z$ 
- 相当于从 $N(0, 1)$ 中采样出一个 $\epsilon$ ，然后 $Z = \mu + \epsilon \sigma$ 

所以重采样的思想就是把复杂分布的采样转化成简单分布的采样，再通过一些变换把采样结果变回去。

# 蒙特卡罗法求定积分

如，需要计算定积分 :
$$
\int_a^b f(x) dx
$$
如果 $f(x)$ 比较简单，直接计算解析解即可。而如果 $f(x)$ 特别复杂，难以求解，就可以借助蒙特卡罗法来近似估计。一个简单的示例如下图所示：

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-205edd92543340306cec8058786b2d6f_720w.webp)

- 为了求 $f(x)$ 在 $[a, b]$ 区间内的积分 $\int_a^b f(x) dx$  
- 首先，找一个更加简单的函数 $g(x)$ 罩在 $f(x)$ 外面，即对于任意 $x \in [a, b]$ ，总有 $g(x) \ge  f(x)$ 。该示例中， $g(x)$ 是 $f(x)$ 上方的那条水平的线段。
- 向 $g(x)$ 在 $[a, b]$ 内的区域随机投点，如果点落在了 $f(x)$ 下方，则接受该点（绿色），否则拒绝该点（红色）。
- 计算接受点的数量 / 总的点的数量 * $g(x)$ 的面积，就可以估计出 $f(x)$ 的面积。
- 其中，由于 $g(x)$ 选择的是非常简单的函数 $g(x) = c$ ，因此其面积 $\int_a^b g(x) dx$ 也非常好算。

---

## 期望法

仍然需要对 $f(x)$ 这个特别复杂的函数算定积分 $\int_a^b f(x) dx$，还可以用期望法计算。

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-d2e55eee623f9931fdb2c7f3410d4feb_720w.webp)

- $[a, b]$ 的值已知，最简单的方法是在 $[a, b]$ 中随机选择一个 $ a \lt x \lt b$ ， 计算 $f(x)$ 。
- 之后，用一个矩形的面积直接粗略估计曲线围成的面积，如下图所示：

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-05ca708b0693047f2ce6b05176af145d_720w.webp)

- 然而，仅采样一个点计算的值过于粗略。一种更精确的改进方式是采样更多的点。如下图所示，采样了4个点：

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-3661ab644db5ac66d34b9f6d087d9271_720w.webp)

- 每个点都单独估计出 $f(x)$ 的定积分，之后对 4 个点算出的面积求平均，就可以得到更加精确的点的面积了

上述过程的数学形式为：

- 为了求解复杂函数的定积分 $\int_a^b f(x) dx$ ，采用期望法，共采样出 $n$ 个 $x$ 点。

- 每次采样算出的面积为 $(a - b) f(x_i)$ ，则 $n$ 次采样的平均面积为 ：
  $$
  S = \frac{1}{n} \sum_{i=1}^n (a -b ) f(x_i) 
  \\=
  \frac{a - b}{n} \sum_{i=1}^n f(x_i)
  $$

上面的示例在采样过程中，默认服从均匀分布采样。而如果从概率分布 $p(x)$ 中进行随机采样，则整个过程为：
$$
\int_a^b f(x) dx = \int_a^b \frac{f(x)}{p(x)} p(x) dx
\\=
\frac{1}{n} \sum_i \frac{f(x_i)}{p(x_i)}
$$
验证：假设 $p(x)$ 为均匀分布，则 $p(x) = \frac{1}{b - a}$ ，计算出的结果一致。

---

更一般的，对于任意复杂的分布 $f(x)$ 计算定积分 $\int_a^b f(x) dx$ ：

- 找到一个间的的分布 $g(x)$ ，如高斯分布。
- 使用一个常数 $C$ 对 $g(x)$ 进行拉伸，保证 $C \times g(x)$ 在$p(x)$ 的上方（同时也希望 $C$ 越小越好，即保证能够罩住 $p(x)$ ，又不能太大），如下图所示：

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-19a1a9400f65eb2b99eb27a5384f62c9_720w.webp)

- 向简单分布（蓝色曲线）围成的区域内投点。如何投点？

  - 根据简单分布，随机采样出样本 $x$  ，并计算 $C \times g(x)$ ：

    ![image-20230627121850381](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/image-20230627121850381.png)

  - 对采样的样本 $(x, C \times g(x))$ 的 $y$ 坐标进行随机移动，使 $y$ 坐标随机变化到上图绿色竖线上的任意一点。该过程可以表示为把纵坐标变换为 $U \times c \times g(x)$ ，$U \in [0, 1]$ 即可实现在上图绿色竖线上移动。

---

算法流程总结为：

- 从建议分布（简单分布） $G$ 中进行采样，得到一个采样样本 $x$ 
- 从均匀分布 $U(0, 1)$ 中进行采样，得到一个采样样本 $u$ 
- 如果 $c \times g(x) \times u < p(x)$ ，即 $u < \frac{p(x)}{g(x) \times c}$ ，则接受采样样本 $x$ ，否则拒绝该采样
- 重复执行上述步骤

# 马尔科夫链蒙特卡罗方法（MCMC）



---

马尔可夫链蒙特卡洛法（Markov Chain Monte Carlo, MCMC） 则是以马尔科夫链为概率模型的蒙特卡罗法。马尔科夫链蒙特卡洛法构建一个马尔科夫链，使其平稳分布就是要进行抽样的分布，首先基于该马尔科夫链进行随机游走，产生样本的序列，之后使用该平稳分布的样本进行近似的数值计算。

马尔科夫链蒙特卡洛法被引用与概率分布的估计，定积分的近似计算，最优化问题的近似求解等问题。

从随机采样方法可以看出，MC需要一个概率分布，如果概率分布比较复杂，就可以使用马尔科夫链进行处理。

## 马尔可夫链

**定义：**考虑一个随机变量的序列 $X = \{ X_0, X_1, ..., X_t, ... \}$ ，这里 $X_t$ 表示时刻 $t$ 的随机变量，$t = 0, 1, 2, ...$ 。每个随机变量 $X_t$ 的取值集合相同，称为状态空间，表示为 $S$ 。随机变量可以是连续的，也可以是离散的，以上随机变量的序列构成随机过程。

- 假设初始时刻的随机变量 $X_0$ 遵循概率分布 $P(X_0) = \pi_0$ ，称为初始状态分布。

- 某个 $t \ge 1$ 时刻的随机变量 $X_t$ 与前一时刻的随机变量 $X_{t-1}$ 之间有条件分布 $P(X_t | X_{t-1})$ 。如果 $X_t$ 只依赖于 $X_{t-1}$ ，而与 $\{ X_0, X_1, ..., X_{t-2} \}$ 都无关，则这一性质称为马尔可夫性：
  $$
  P(X_t|X_{t-1}, X_{t-2}, ..., X_0) = P(X_t|X_{t-1})
  $$

- 具有马尔可夫性的随机序列 $X = \{ X_0, X_1, ..., X_t, ... \}$ 称为马尔科夫链，或马尔可夫过程。

- 条件概率分布 $P(X_t|X_{t-1})$ 称为马尔可夫链的转移概率分布。转移概率分布决定了马尔可夫链的特性。

- 如果这个**条件概率分布**与**具体的时刻** $t$ 是无关的，则称这个马尔科夫链为时间齐次的马尔可夫链(time homogenous Markov chain)。

---

### 转移概率矩阵

 **定义，**转移概率矩阵：
$$
P = 
\left[\begin{array}{c}
p_{11} & p_{12} & p_{13} \\
p_{21} & p_{22} & p_{23} \\
p_{31} & p_{32} & p_{33} \\
\end{array}\right]
$$
其中，$p_{ij} = P(X_t = i | X_{t-1} = j) $

---

转移概率矩阵的示例如下：



如下图王者荣耀玩家选择英雄的职业的转变，转移概率矩阵为：
$$
P = 
\left[\begin{array}{c}
0.5 & 0.5 & 0.25 \\
0.25 & 0 & 0.25 \\
0.25 & 0.5 & 0.5 \\
\end{array}\right]
$$
其中，状态 $1,2,3$ 分别表示玩 射手，辅助 和 打野。

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-696651b37f2dc6a472c314f1ba78194d_720w.webp)

则：

- $P(x_t = 1, x_{t-1} = 1) = 0.5 $ 表示初始玩射手，下一次还玩射手的概率是 0.5
- $P(x_t = 1, x_{t-1} = 2) = 0.5 $ 表示初始玩辅助，下一次玩射手的概率是 0.5
- $P(x_t = 1, x_{t-1} = 3) = 0.5 $ 表示初始玩打野，下一次玩射手的概率是 0.25
- $P(x_t = 1, x_{t-1} = 3) = 0.5 $ 表示初始玩打野，下一次玩射手的概率是 0.25
- 上面的图和概率转移矩阵有点问题，因为图是网上找的。其中辅助到射手，即$p(3,2)$ 应该是0.5

---

上面的例子是已经知道了初始玩什么了，然后估计下一次选的角色。但是如果现在刚打开手机，即 $t=0$ 时应该对应哪个角色呢？所以还需要一个初始状态分布，如下一小节所述。

这里如果假设初始状态玩三个角色的概率分布为 $[0.5, 0.3, 0.2]$ ，则按照上面的转移概率矩阵， $t=100$ 时所玩角色的状态分布为：

```python
import numpy as np
import matplotlib.pyplot as plt

transfor_matrix = np.array([
    [0.5, 0.5, 0.25],
    [0.25, 0, 0.25],
    [0.25, 0.5, 0.5]
])
status = np.array([[0.5, 0.3, 0.2]]).reshape([-1, 1])

t = 100

prob_sheshou = [status[0]]
prob_fuzhu = [status[1]]
prob_daye = [status[2]]
for _ in range(t):
    status = transfor_matrix.dot(status)

    prob_sheshou.append(status[0])
    prob_fuzhu.append(status[1])
    prob_daye.append(status[2])
print(status)

plt.plot(np.arange(t + 1), prob_sheshou, label='sheshou')
plt.plot(np.arange(t + 1), prob_fuzhu, label='fuzhu')
plt.plot(np.arange(t + 1), prob_daye, label='daye')
plt.legend()
plt.show()
```

- 100次之后的状态分布（下节定义的内容）如下：

![image-20231217012435053](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/image-20231217012435053.png)

### 状态分布

定义：马尔科夫链在 $t$ 时刻的概率分布被称为 $t$ 时刻的状态分布：
$$
\pi(t) =
\left[\begin{array}{c}
\pi_1(t) \\
\pi_2(t) \\
\pi_3(t) \\
\end{array}\right]
$$
其中，$\pi_i(t) = P(X_t = i), i=1,2,...$ 

特别的，马尔科夫链的初始状态分布可以表示为：
$$
\pi(0) =
\left[\begin{array}{c}
\pi_1(0) \\
\pi_2(0) \\
\pi_3(0) \\
\end{array}\right]
$$
通常初始分布 $\pi(0)$ 向量只有一个分量是1，其余分量是0，表示马尔科夫链从一个具体状态开始。

### 马尔科夫链的性质

#### 不可约 && 可约

定义：

给定一个马尔科夫链 $X = \{ X_0, X_1, ..., X_t, ... \}$ ，对于任意的状态 $i, j \in S$ ，如果存在一个时刻 $t$ 满足 $P(X_t = i | X_0 = j) > 0$ ，也就是说，时刻 $0$ 从状态 $j$ 出发，时刻 $t$ 到达状态 $i$ 的概率大于0，则称此马尔科夫链 $X$ 是不可约的，否则称马尔科夫链是可约的。

- 简单来说，一个不可约的马尔可夫链，从任意状态出发，当经过充分长时间后，可以到达任意状态。

#### 非周期 && 周期

定义：

给定一个马尔科夫链 $X = \{ X_0, X_1, ..., X_t, ... \}$ ，对于任意状态 $i \in S$ ，如果时刻 $0$ 从状态 $i$ 出发，$t$ 时刻返回状态的所有时间长 $\{ t: P(X_t = i | X_0 = i) > 0\}$ 的最大公约数是1，则称此马尔科夫链 $X$ 是非周期的，否则称马尔科夫链是周期的。  

### 一些定义

#### 首达时间

$T_{ij} = min\{ n:n\ge1, X_0 = i, X_n = j \}$ 表示从状态 $i$ 出发首次达到状态 $j$ 的时间。若状态 $i$ 出发永远不能到达状态 $j$ ，则 $T_{ij} = + \infty$ 。

#### 首达概率

定义首达概率 $f_{ij}^{(n)} = P(X_n = j, X_m \ne j, m = 1,2,..,n-1|X_0 = i)$ ：

- $f_{ij}^{(n)}$ 为从状态 $i$ 出发经过 $n$ 步首次到达状态 $j$ 的概率
- $f_{ij}^{(+ \infty)}$ 为从状态 $i$ 出发永远不能到达状态 $j$ 的概率

#### **从状态 $i$ 出发经过有限步首次到达状态 $j$ 的概率**

$$
f_{ij} = \sum_{n=1}^{+ \infty} f_{ij}^{(n)} = P(T_{ij} < + \infty)
$$

- 个人理解，$f_{ij} = f_{ij}^{(0)} + f_{ij}^{(1)} + ... + f_{ij}^{(n)} = 0+0+0+0 + ... + f_{ij}^{(n)}$ 
- 如果 $f_{ii} = 1$ ，则有限步一定能回来
- 如果 $f_{ii} < 1$ ，则有限步可能回不来

#### 平均返回时间

$$
\mu_i = \sum_{n = 1}^{+\infty} nf_{ii}^{(n)}
$$

 即，时间 $n$ 的期望。

#### 正常反 && 零常反

- 如果平均返回时间 $\mu_i < +\infty$ ，则称状态 $i$ 是正常反
- 如果平均返回时间 $\mu_i = +\infty$ ，则称状态 $i$ 是零常反

#### 遍历态

- $i$ 既是正常反又是非周期，就是遍历态。

---

![img](imgs/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7/v2-4ac8de5653c49ccc2aede6dd9d535a18_720w.webp)

直观上，一个正常返的马尔可夫链，其中任意一个状态，从其他任意一个状态出发，当时间趋于无穷时，首次转移到这个状态的概率不为0。(从任意一个状态出发，走了能回来)

## MCMC

一般的采样问题，以及期望求解，数值近似问题，蒙特卡罗方法都能很好地解决；但遇到多元变量的随机分布以及复杂的概率密度时，仅仅使用蒙特卡罗方法就会显得捉襟见肘，这时就需要这篇文章要讲的马尔可夫链蒙特卡罗法来解决这个问题了。

== 

https://zhuanlan.zhihu.com/p/250146007

https://zhuanlan.zhihu.com/p/253784711
