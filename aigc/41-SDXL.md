# SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis

![image-20231214181802597](./imgs/41-SDXL/image-20231214181802597.png)

# 0 引用

- 论文：https://arxiv.org/pdf/2307.01952.pdf



# 1 介绍

- 比SD1.5的UNet大了3倍 --- 主要来自于永乐更多的attention block，并增加了一个新的 text encoder 所带来的维度开销。
- 设计了多个新的条件编码策略

- 在多个高宽比上训练SDXL
- 引入一个refinement model来改善生成质量，refiner是图生图的方式

# 2 方法

![image-20231214191954750](./imgs/41-SDXL/image-20231214191954750.png)

## 2.1 Architecture & Scale

![image-20231214194226082](./imgs/41-SDXL/image-20231214194226082.png)

- 扩散模型底层架构的发展：添加自注意力机制 -> 交叉注意力机制 -> 完全的transformer结构。SDXL也遵循了这种趋势。
- 出于效率考虑：
  - 移除了浅层网络的transformer blocks，在深层网络中使用 2 / 10个transofmer blocks。
  - 移除了8倍下采样的Unet层（深层特征的通道太大）
- 使用了ViT-bigG 和  ViT-L两个CLIP，在channel维度concat两个te的特征。
- 除了使用交叉注意力机制来注入文本编码，还使用了CLIP的 pooled text embedding特征。

## 2.2 Micro - Conditioning

### 2.2.1 Image Size

- 扩散模型由于其两阶段的架构，训练模型需要保证一个最小分辨率的图像尺寸。为了解决这个问题：

  - SD1.5 和 SD1.4 丢弃所有尺寸低于512的图像，但是这种方法会导致很大一部分的训练数据被丢弃，影响模型的泛化能力。如下图所示，如果仅丢弃分辨率小于 $256 \times 256$ 的图像，就会丢弃 $39\%$ 的训练数据。 

    ![image-20231216135117743](imgs/41-SDXL/image-20231216135117743.png)

  - 其他一些方法有些上采样图像，这种方法通常会引入伪影，导致模型训练时学习到伪影，导致生成的样本模糊。

因此，SDXL把图像尺寸也当成了UNet模型的条件：

- 原始高宽（在做任何缩放之前的高宽）$c_{size} = (h_{original}, w_{original})$ ，高宽分别使用傅里叶特征编码，之后concat成一个特征向量 $[1 \times 512]  $ ，之后有一个projection层。最终加到 timestep embedding上去。
- 推理时，也需要制定想要的尺寸。推理时尺寸条件的影响如下图所示，所有图像都是 $512 \times 512$ 生成的。

![image-20231216135508016](imgs/41-SDXL/image-20231216135508016.png) 

### 2.2.2 Cropping Parameters

![image-20231216140117916](imgs/41-SDXL/image-20231216140117916.png)

如上图所示，SD模型一个经典的缺点就是物体会只生成部分。这个问题是由于Pytorch需要batch内的所有图像都有相同的尺寸导致的，常见的解决方式先resize再crop。

为了解决该问题，SDXL使用了 crop coordinates作为条件：

- 训练时，均匀的采样出crop的左上角坐标$c_{top}, c_{left}$ 

- 后续的编码等和size的相同

- 推理时crop坐标是0，0

采样crop左上角坐标的算法如下：

![image-20231216142551402](imgs/41-SDXL/image-20231216142551402.png)

虽然数据分桶也可以减轻直接crop的问题，但SDXL实验发现，crop条件仍然有增，并且能够crop条件实现对数据的更多控制。并且，crop也不需要提前对数据进行预处理。crop条件再推理时的差别如下图所示。

![image-20231216143122398](imgs/41-SDXL/image-20231216143122398.png)

## 2.4 Multi-Aspect-Training

模型通常用 $512 \times 512$ 或 $1024 \times 1024$ 固定尺度训练，灵活性太低。SDXL为了提升尺度的灵活性：

- 选择高宽乘积接近 $1024^2$ 的图像

- 已 64 为 step 分桶，最终分桶的训练数据尺寸如下图所示：

  ![image-20231216144127086](imgs/41-SDXL/image-20231216144127086.png)

- 每个batch的训练数据都来自于同一个桶，每个step替换一个桶。
- 模型还接收了桶的尺寸（或目标尺寸）作为条件 $c_{ar} = (h_{tgt}, w_{tgt})$ 。加入方式与size和crop相同。

多尺度训练只再最后的微调阶段进行。

## 2.5 Impreved Autoencoder

使用更大的batch-size （16 vs. 9）重新训练了 VAE。并使用了 EMA 。效果对比如下图所示：

![image-20231216144549343](imgs/41-SDXL/image-20231216144549343.png)

# 3 训练

通用设置：

- $T = 1000$

- 数据集来自于网络，高宽分布：

  ![image-20231216144836237](imgs/41-SDXL/image-20231216144836237.png)

训练分多个阶段：

- $256 \times 256$ 训练
  - bs = 2048
  - 600000个优化步
  - size 和 crop作为条件
- $512 \times 512$ 
  - 额外的 200000 个优化步
- 多尺度训练
  - noise offset = 0.05

# 4 Refinement Stage

![image-20231216145657701](imgs/41-SDXL/image-20231216145657701.png)

实验发现，最终的模型有时出图质量较低，如上图所示。为解决该问题，在相同的隐空间上训练了一个额外的LDM模型。

- Reifner使用高质量和高分辨率图像，使用SDEdit中的去噪过程。
- Refiner制定模型只有200个噪声尺度。
- 推理时，render base模型的隐变量，并直接在隐空间进行去噪
- Refiner使用与base相同的文本输入。
- Refiner不是必须的，但是对于背景细节和人脸的改善有用。

# 5 Limitations

![image-20231216150209101](imgs/41-SDXL/image-20231216150209101.png)

- 在合成复杂的结构（如人手）时可能效果较差，如上图 top-left 。
  - 尽管XL使用了各种数据训练，但是人体解刨学的复杂性结构给实现准确的表示带来了困难。这种限制表明需要进一步的缩放和训练技术，针对专门细粒度的细节合成。发生这种情况的原因可能是手和类似物体在照片中出现的差异非常大，在这种情况下模型很难提取真实 3D 形状和物理限制的知识。
- 虽然该模型在生成的图像中实现了显着的真实感水平，但值得注意的是，它并没有达到完美的照片真实感。某些细微差别，例如微妙的灯光效果或微小的纹理变化，可能仍然不存在或在生成的图像中不太忠实地表示。此限制意味着在需要高度视觉保真度的应用程序中仅依赖模型生成的视觉效果时应谨慎行事。
- 此外，该模型的训练过程严重依赖于大规模数据集，这可能会无意中引入社会和种族偏见。因此，模型在生成图像或推断视觉属性时可能会无意中加剧这些偏差。
- 模型存在概念溢出的问题（concept bleeding），如橙色的毛衣图像中把太阳也变成橙色了。识别和解决此类事件对于提高模型准确分离和表示复杂场景中单个对象的能力至关重要。其根本原因可能在于使用的预训练文本编码器：首先，它们被训练为将所有信息压缩为单个标记，因此它们可能无法仅绑定正确的属性和对象。
- 文本生成问题。虽然我们的模型比之前的 SD 迭代取得了显着进步，但在渲染长而清晰的文本时仍然遇到困难。有时，生成的文本可能包含随机字符或表现出不一致。克服这一限制需要进一步研究和开发增强模型文本生成能力的技术，特别是对于扩展的文本内容

