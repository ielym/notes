# 1 介绍

目的是提升regression-based方法的性能，达到能够与heatmap-based方法接近的效果。与heatmap-based方法相比，基于回归的方法由很多优点：

- 具有更低的计算和存储复杂度

- 不存在量化问题，输出是连续的
- 可以以最小的成本扩招到各种应用场景，如但阶段方法，基于视频的方法，以及3D场景

然而，现有基于回归的方法的性能较差，限制了其使用。

本文提出了 Residual Log-likelihood Estimation (RLE) 方法。

# 2 方法

## 2.1 一般的回归方法

标准的回归范式使用 $l_1, l_2$ 损失来回归输出 $\hat{\mu}$ 。本文首先从极大似然估计的角度来重新审视回归方法：

- 给定输入图像 $I$ ，回归方法预测一种分布$P_{\Theta} (x | I)$ ，表示gt位置 $\mu_g$ 出现在位置 $x$ 的概率，$\Theta$ 表示可学习的模型参数。

- 由于固有的歧义，存在标注误差，因此 gt $\mu_g$ 实际上可以认为是在真正 gt 附近的一个观察值。在优化 $\Theta$ 的过程中，实际上是为了使观察到 $\mu_g$ 的可能性最大。

- 因此，上述是极大似然的思想：
  $$
  L_{mle} = - log P_{\Theta} (x | I) |_{x = \mu_g}
  $$

- 极大似然估计的前提是分布已知。对于不同的损失函数，实际上是对输出分布的不同假设。如：

  - 在目标检测中，一些方法假设该分布是高斯分布，模型需要预测 $\hat{\mu}， \hat{\sigma}$ ，并构造概率密度函数 $P_{\Theta}(x | I) = \frac{1}{\sqrt{2\pi} \hat{\sigma}} exp(- \frac{(x - \hat{\mu})^2}{2 \hat{\sigma}^2})$ ，为了最大化高斯分布的极大似然估计，需要优化模型，使得观察到标签样本 $\mu_g$ 的可能性最大，因此损失函数为：
    $$
    L = - log(P_\Theta (x | I)) |_{x - \mu_g} \propto log \hat{\sigma} + \frac{(\mu_g - \hat{\mu})^2}{2\hat{\sigma}^2}
    $$

  - 如果假设密度函数由固定的方差，如 $\hat{\sigma}$ 是常数。则损失函数就退化成了 $l_2$ Loss：$L = (\mu_g - \hat{\mu})^2$

  - 此外，如果假设该分布式拉普拉斯分布，则损失函数就变成了 $l_1$ loss

  - 在推理过程中，模型预测的 $\hat{\mu}$ 被用于控制分布的位置，作为回归输出。

- 从上述可是，损失函数依赖于假设的位置分布的密度函数。然而，高斯分布或拉普拉斯分布都是提前假设的，可能并不准确。因此，如果使用更准确的分布进行估计，则直觉上应该能获得更好的效果。所以本文提出了一种新的回归范式，使用 normalizing flow

## 2.2 Normalizing Flows

### 2.2.1 Basic Design

