# 1 背景

- self-attention的运算是无向的，无法区分不同token所在的位置，以及token之间的相对位置关系。

## 1.1 几种可能的位置编码方式

- **用整形值标记位置**
  - 第一个token标记1，第二个token标记2, ... 
  - 存在问题：
    - 推理时可能遇到比训练数据更长的序列，泛化性不足。
    - 位置编码是无界的
- **用 [0, 1] 区间的值进行标记**
  - 等间隔或其他策略在 $[0, 1]$ 区间进行划分。如，$[0, 1], [0, 0.5, 1], [0, 0.33, 0.69, 1]$ 
  - 存在问题：
    - 当序列长度不同时，这种方式会导致不同token之间的相对位置变化。
- 用二进制方式进行标记
  - 常见的pos emb都使用相加的方式注入到embedding中，因此需要保证pos emb的维度和embedding的维度相同。通常embedding的维度较大，因此可以表示的位置范围也比较大，基本是可行的方案。
  - 如，$(0, 0), (0, 1), (1, 0), (1, 1)$ 分别表示索引为 0, 1, 2, 3的4个token的位置编码
  - 存在问题：
    - 位置编码是离散的，信息容量有限

---

因此满足要求的位置编码方式应该具有的条件包括：

- 能够表示token的绝对位置
- 当序列变化时，不同token之间的相对位置需要保持一致
- 可以扩展到训练过程中没有见过的序列长度

# Sine-Cosine Positional Embedding

正余弦函数天然具有的特性：

- 连续
- 有界
- 周期性

